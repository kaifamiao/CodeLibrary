web-crawler-multithreaded
给你一个初始地址<code>startUrl</code>和一个 HTML 解析器接口<code>HtmlParser</code>，请你实现一个<strong>多线程的网页爬虫</strong>，用于获取与<code>startUrl</code>有<strong>相同主机名</strong>的所有链接。

以<strong>任意</strong>顺序返回爬虫获取的路径。

爬虫应该遵循：

<ul>
	从<code>startUrl</code>开始
	调用<code>HtmlParser.getUrls(url)</code> 从指定网页路径获得的所有路径。
	不要抓取相同的链接两次。
	仅浏览与<code>startUrl</code><strong>相同主机名</strong>的链接。
</ul>

<img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e463265c7086cb?w=975&amp;h=266&amp;f=png&amp;s=24624" /><img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/urlhostname.png" style="height:164px; width:600px" />

如上图所示，主机名是<code>example.org</code>。简单起见，你可以假设所有链接都采用<strong>http 协议</strong>，并且没有指定<strong>端口号</strong>。举个例子，链接<code>http://leetcode.com/problems</code> 和链接<code>http://leetcode.com/contest</code> 属于同一个<strong>主机名</strong>， 而<code>http://example.org/test</code>与<code>http://example.com/abc</code> 并不属于同一个<strong>主机名</strong>。

<code>HtmlParser</code> 的接口定义如下：

<pre>
interface HtmlParser {
  // Return a list of all urls from a webpage of given <em>url</em>.
  // This is a blocking call, that means it will do HTTP request and return when this request is finished.
  public List&lt;String&gt; getUrls(String url);
}</pre>

注意一点，<code>getUrls(String url)</code>模拟执行一个HTTP的请求。 你可以将它当做一个阻塞式的方法，直到请求结束。<code>getUrls(String url)</code>保证会在<strong>15ms</strong>内返回所有的路径。 单线程的方案会超过时间限制，你能用多线程方案做的更好吗？

对于问题所需的功能，下面提供了两个例子。为了方便自定义测试，你可以声明三个变量<code>urls</code>，<code>edges</code>和<code>startUrl</code>。但要注意你只能在代码中访问<code>startUrl</code>，并不能直接访问<code>urls</code>和<code>edges</code>。



<strong>拓展问题：</strong>

<ol>
	假设我们要要抓取 10000 个节点和 10 亿个路径。并且在每个节点部署相同的的软件。软件可以发现所有的节点。我们必须尽可能减少机器之间的通讯，并确保每个节点负载均衡。你将如何设计这个网页爬虫？
	如果有一个节点发生故障不工作该怎么办？
	如何确认爬虫任务已经完成？
</ol>



<strong>示例 1：</strong>

<img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/sample_2_1497.png" style="height:287px; width:600px" /><img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e46559da0c446a?w=875&amp;h=418&amp;f=png&amp;s=43518" />

<pre>
<strong>输入：
</strong>urls = [
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.google.com&quot;,
 &quot;http://news.yahoo.com/us&quot;
]
edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
<strong>输出：</strong>[
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.yahoo.com/us&quot;
]
</pre>

<strong>示例 2：</strong>

<strong><img alt="" src="https://user-gold-cdn.xitu.io/2019/11/7/16e4657b399a5fd2?w=654&amp;h=431&amp;f=png&amp;s=33838" /><img alt="" src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2019/11/09/sample_3_1497.png" style="height:395px; width:530px" /></strong>

<pre>
<strong>输入：</strong>
urls = [
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.google.com&quot;
]
edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
startUrl = &quot;http://news.google.com&quot;
<strong>输出：</strong>[&quot;http://news.google.com&quot;]
<strong>解释：</strong>startUrl 链接与其他页面不共享一个主机名。</pre>



<strong>提示：</strong>

<ul>
	<code>1 &lt;= urls.length &lt;= 1000</code>
	<code>1 &lt;= urls[i].length &lt;= 300</code>
	<code>startUrl</code>是<code>urls</code>中的一个。
	主机名的长度必须为 1 到 63 个字符（包括点 <code>.</code> 在内），只能包含从 &ldquo;a&rdquo; 到 &ldquo;z&rdquo; 的 ASCII 字母和 &ldquo;0&rdquo; 到 &ldquo;9&rdquo; 的数字，以及中划线 &ldquo;-&rdquo;。
	主机名开头和结尾不能是中划线 &ldquo;-&rdquo;。
	参考资料：<a href="https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames">https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames</a>
	你可以假设路径都是不重复的。
</ul>

多线程网页爬虫
Given a url <code>startUrl</code> and an interface <code>HtmlParser</code>, implement <strong>a Multi-threaded webcrawler</strong> to crawl all links that are under the<strong>same hostname</strong> as<code>startUrl</code>.

Returnall urls obtained by your web crawler in <strong>any</strong> order.

Your crawler should:

<ul>
	Start from the page: <code>startUrl</code>
	Call <code>HtmlParser.getUrls(url)</code> to get all urls from a webpage of given url.
	Do not crawl the same link twice.
	Explore only the links that are under the <strong>same hostname</strong> as <code>startUrl</code>.
</ul>

<img alt="" src="https://assets.leetcode.com/uploads/2019/08/13/urlhostname.png" style="width: 600px; height: 164px;" />

As shown in the example url above, the hostname is <code>example.org</code>. For simplicity sake, you may assume allurls use <strong>http protocol</strong> without any<strong>port</strong> specified. For example, the urls<code>http://leetcode.com/problems</code> and<code>http://leetcode.com/contest</code> are under the same hostname, while urls <code>http://example.org/test</code> and <code>http://example.com/abc</code> are not under the same hostname.

The <code>HtmlParser</code> interface is defined as such:

<pre>
interface HtmlParser {
  // Return a list of all urls from a webpage of given <em>url</em>.
  // This is a blocking call, that means it will do HTTP request and return when this request is finished.
  public List&lt;String&gt; getUrls(String url);
}</pre>

Note that<code>getUrls(String url)</code>simulates performing aHTTP request. You can treat it as a blocking function call which waits for aHTTP request to finish. It is guaranteed that<code>getUrls(String url)</code> will return the urls within <strong>15ms.</strong> Single-threaded solutions will exceed the time limit so, can your multi-threaded web crawler do better?

Beloware two examples explaining the functionality of the problem, for custom testing purposes you&#39;ll have threevariables<code data-stringify-type="code">urls</code>,<code data-stringify-type="code">edges</code>and<code data-stringify-type="code">startUrl</code>. Notice that you will only have access to<code data-stringify-type="code">startUrl</code>in your code, while<code data-stringify-type="code">urls</code>and<code data-stringify-type="code">edges</code>are not directly accessible to you in code.



<strong>Follow up:</strong>

<ol>
	Assume we have 10,000 nodes and 1 billion URLs to crawl.We will deploy the same software onto each node.The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?
	What if one node fails or does not work?
	How do you know when the crawler is done?
</ol>


<strong>Example 1:</strong>

<img alt="" src="https://assets.leetcode.com/uploads/2019/10/23/sample_2_1497.png" style="width: 610px; height: 300px;" />

<pre>
<strong>Input:
</strong>urls = [
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.google.com&quot;,
 &quot;http://news.yahoo.com/us&quot;
]
edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
<strong>Output:</strong> [
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.yahoo.com/us&quot;
]
</pre>

<strong>Example 2:</strong>

<strong><img alt="" src="https://assets.leetcode.com/uploads/2019/10/23/sample_3_1497.png" style="width: 540px; height: 270px;" /></strong>

<pre>
<strong>Input:</strong> 
urls = [
 &quot;http://news.yahoo.com&quot;,
 &quot;http://news.yahoo.com/news&quot;,
 &quot;http://news.yahoo.com/news/topics/&quot;,
 &quot;http://news.google.com&quot;
]
edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
startUrl = &quot;http://news.google.com&quot;
<strong>Output:</strong> [&quot;http://news.google.com&quot;]
<strong>Explanation: </strong>The startUrl links to all other pages that do not share the same hostname.</pre>


<strong>Constraints:</strong>

<ul>
	<code>1 &lt;= urls.length &lt;= 1000</code>
	<code>1 &lt;= urls[i].length &lt;= 300</code>
	<code>startUrl</code>is one of the <code>urls</code>.
	Hostname label must be from 1 to 63 characters long, including the dots, may contain only the ASCII letters from &#39;a&#39; to&#39;z&#39;, digits from &#39;0&#39; to &#39;9&#39; and thehyphen-minuscharacter (&#39;-&#39;).
	The hostname may not start or end withthe hyphen-minus character (&#39;-&#39;).
	See:<a href="https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames">https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames</a>
	You may assume there&#39;reno duplicates in url library.
</ul>
